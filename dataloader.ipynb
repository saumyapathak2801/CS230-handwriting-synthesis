{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    def __init__(self, args):\n",
    "        self.rootDir = \"./data\"\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.index_pointer = 0\n",
    "        self.timesteps = args['tsteps']\n",
    "        self.scale_factor = args['scale_factor']\n",
    "        self.gap = args['gap']\n",
    "        \n",
    "        \n",
    "        stroke_dir = self.rootDir\n",
    "        data_file = os.path.join(self.rootDir, \"strokes_training_data.cpkl\")\n",
    "        self.process(stroke_dir, data_file)\n",
    "        self.read_processed(data_file)\n",
    "        self.init_batch_comp()\n",
    "    \n",
    "    # Read processed data from .cpkl file.\n",
    "    def read_processed(self, data_file):\n",
    "        # Opening in read mode\n",
    "        f = open(data_file, 'rb')\n",
    "        self.raw_stroke_data = pickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.valid_stroke_data = []\n",
    "        self.stroke_data = []\n",
    "        \n",
    "        for i in range(len(self.raw_stroke_data)):\n",
    "            data = self.raw_stroke_data[i]\n",
    "            if (len(data) > self.timesteps + 2):\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.gap)\n",
    "                data = np.maximum(data, -self.gap)\n",
    "                data[:,0:2] /= self.scale_factor\n",
    "                if i%20 == 0:\n",
    "                    self.valid_stroke_data.append(data)\n",
    "                else:\n",
    "                    self.stroke_data.append(data)\n",
    "        self.num_batches = int(len(self.stroke_data)/self.batch_size)\n",
    "        print(\"Number of data examples:\",  len(self.stroke_data))\n",
    "        print(\"Batch size for dataset\", self.num_batches)\n",
    "\n",
    "        \n",
    "    def init_batch_comp(self):\n",
    "        self.index_perm = np.random.permutation(len(self.stroke_data))\n",
    "        self.index_pointer = 0\n",
    "    \n",
    "    def get_next_batch(self):\n",
    "        # Iterate for batch_size times to get a batch of batch_size points\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            # Pick strokes data randomly from each file\n",
    "            data = self.stroke_data[self.index_perm[self.index_pointer]]\n",
    "            x_batch.append(np.copy(data[:self.timesteps]))\n",
    "            y_batch.append(np.copy(data[1:self.timesteps+1]))\n",
    "            self.index_pointer += 1\n",
    "            if(self.index_pointer >= len(self.stroke_data)):\n",
    "                self.init_batch_comp()          \n",
    "        return x_batch, y_batch         \n",
    "            \n",
    "        \n",
    "    def process(self, rootDir, data_file):\n",
    "        # Function that outputs linestrokes given filepath.    \n",
    "        def convert_linestroke_file_to_array(filepath):\n",
    "            strokeFile = ET.parse(filepath)\n",
    "            root = strokeFile.getroot()\n",
    "            x_min_offset = -1000000\n",
    "            y_min_offset = -1000000\n",
    "            y_height = 0\n",
    "            for i in range(1,4):\n",
    "                x_min_offset = min(x_min_offset, float(root[0][i].attrib['x']))\n",
    "                y_min_offset = min(y_min_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            #TODO(add normalization)\n",
    "            y_height -= y_min_offset\n",
    "            x_min_offset -=100.0\n",
    "            y_min_offset -=100.0\n",
    "            strokeSet = root[1]\n",
    "            allStrokes = []\n",
    "            for i in range(len(strokeSet)):\n",
    "                points = []\n",
    "                for point in strokeSet[i]:\n",
    "                    points.append([(float(point.attrib['x']) - x_min_offset), (float(point.attrib['y']) - y_min_offset)])\n",
    "                allStrokes.append(points)\n",
    "            return allStrokes    \n",
    "                \n",
    "    \n",
    "        def get_all_files():\n",
    "            rootDir = \"./data\"\n",
    "            filePaths = []\n",
    "            for dirpath, dirnames, filenames in os.walk(rootDir):\n",
    "                for file in filenames:\n",
    "                    filePaths.append(os.path.join(dirpath, file))\n",
    "            return filePaths\n",
    "\n",
    "        \n",
    "    # Function to convert strokes to inputStrokeMatrix\n",
    "        def strokes_to_input_matrix(strokes):\n",
    "            strokeMatrix = []\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            for stroke in strokes:\n",
    "                for num_point in range(len(stroke)):\n",
    "                    x = stroke[num_point][0] - prev_x\n",
    "                    y = stroke[num_point][1] - prev_y\n",
    "                    prev_x = stroke[num_point][0]\n",
    "                    prev_y = stroke[num_point][1]\n",
    "                    z = 0\n",
    "                    if (num_point == len(stroke)-1):\n",
    "                        z = 1\n",
    "                    example = [x,y,z]\n",
    "                    strokeMatrix.append(example)\n",
    "            return strokeMatrix\n",
    "        \n",
    "        allFiles = get_all_files()\n",
    "        strokes = []\n",
    "        counter = 0\n",
    "        for file in allFiles:\n",
    "            if file[-3:] == \"xml\" and 'a' in file:\n",
    "                counter = counter + 1\n",
    "                stroke = strokes_to_input_matrix(convert_linestroke_file_to_array(file))\n",
    "                strokes.append(stroke)\n",
    "            assert len(strokes) == counter    \n",
    "        f = open(data_file,\"wb\")\n",
    "        pickle.dump(strokes, f, protocol=2)\n",
    "        f.close()\n",
    "        print(\"Saved {} lines\", len(strokes))\n",
    "        \n",
    "def to_one_hot(s, ascii_steps, alphabet):\n",
    "    steplimit=3e3; s = s[:3e3] if len(s) > 3e3 else s # clip super-long strings\n",
    "    seq = [alphabet.find(char) + 1 for char in s]\n",
    "    if len(seq) >= ascii_steps:\n",
    "        seq = seq[:ascii_steps]\n",
    "    else:\n",
    "        seq = seq + [0]*(ascii_steps - len(seq))\n",
    "    one_hot = np.zeros((ascii_steps,len(alphabet)+1))\n",
    "    one_hot[np.arange(ascii_steps),seq] = 1\n",
    "    return one_hot\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
