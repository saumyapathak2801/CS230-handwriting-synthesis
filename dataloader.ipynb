{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    def __init__(self, args):\n",
    "        self.rootDir = \"./data\"\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.index_pointer = 0\n",
    "        self.timesteps = args['tsteps']\n",
    "        self.scale_factor = args['scale_factor']\n",
    "        self.gap = args['gap']\n",
    "        self.char_steps = args['tsteps']/args['tsteps_per_char']\n",
    "        self.alphabet = args['alphabet']\n",
    "        \n",
    "        \n",
    "        stroke_dir = self.rootDir + \"/lineStrokes\"\n",
    "        ascii_dir = self.rootDir + \"/ascii\"\n",
    "        data_file = os.path.join(self.rootDir, \"strokes_training_data.cpkl\")\n",
    "        self.process(stroke_dir, ascii_dir, data_file)\n",
    "        self.read_processed(data_file)\n",
    "        self.init_batch_comp()\n",
    "    \n",
    "    # Read processed data from .cpkl file.\n",
    "    def read_processed(self, data_file):\n",
    "        # Opening in read mode\n",
    "        f = open(data_file, 'rb')\n",
    "        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.valid_stroke_data = []\n",
    "        self.stroke_data = []\n",
    "        self.ascii_data = []\n",
    "        self.valid_ascii_data = []\n",
    "        \n",
    "        for i in range(len(self.raw_stroke_data)):\n",
    "            data = self.raw_stroke_data[i]\n",
    "            if (len(data) > self.timesteps + 2):\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.gap)\n",
    "                data = np.maximum(data, -self.gap)\n",
    "                data[:,0:2] /= self.scale_factor\n",
    "                if i%20 == 0:\n",
    "                    self.valid_stroke_data.append(data)\n",
    "                    self.valid_ascii_data.append(self.raw_ascii_data[i])\n",
    "                else:\n",
    "                    self.stroke_data.append(data)\n",
    "                    self.ascii_data.append(self.raw_ascii_data[i])\n",
    "        self.num_batches = int(len(self.stroke_data)/self.batch_size)\n",
    "        print(\"Number of data examples:\",  len(self.stroke_data))\n",
    "        print(\"Batch size for dataset\", self.num_batches)\n",
    "\n",
    "        \n",
    "    def init_batch_comp(self):\n",
    "        self.index_perm = np.random.permutation(len(self.stroke_data))\n",
    "        self.index_pointer = 0\n",
    "    \n",
    "    def get_next_batch(self):\n",
    "        # Iterate for batch_size times to get a batch of batch_size points\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        asciis = []\n",
    "        for i in range(self.batch_size):\n",
    "            # Pick strokes data randomly from each file\n",
    "            data = self.stroke_data[self.index_perm[self.index_pointer]]\n",
    "            x_batch.append(np.copy(data[:self.timesteps]))\n",
    "            y_batch.append(np.copy(data[1:self.timesteps+1]))\n",
    "            asciis.append(self.ascii_data[self.index_perm[self.index_pointer]])\n",
    "            self.index_pointer += 1\n",
    "            if(self.index_pointer >= len(self.stroke_data)):\n",
    "                self.init_batch_comp()\n",
    "            asciis_oh = [convert_to_one_hot(s, self.char_steps, self.alphabet) for s in asciis]    \n",
    "        return x_batch, y_batch, asciis, asciis_oh         \n",
    "            \n",
    "        \n",
    "    def process(self, rootDir, asciiDir, data_file):\n",
    "        # Function that outputs linestrokes given filepath.    \n",
    "        def convert_linestroke_file_to_array(filepath):\n",
    "            strokeFile = ET.parse(filepath)\n",
    "            root = strokeFile.getroot()\n",
    "            x_min_offset = -1000000\n",
    "            y_min_offset = -1000000\n",
    "            y_height = 0\n",
    "            for i in range(1,4):\n",
    "                x_min_offset = min(x_min_offset, float(root[0][i].attrib['x']))\n",
    "                y_min_offset = min(y_min_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            #TODO(add normalization)\n",
    "            y_height -= y_min_offset\n",
    "            x_min_offset -=100.0\n",
    "            y_min_offset -=100.0\n",
    "            strokeSet = root[1]\n",
    "            allStrokes = []\n",
    "            for i in range(len(strokeSet)):\n",
    "                points = []\n",
    "                for point in strokeSet[i]:\n",
    "                    points.append([(float(point.attrib['x']) - x_min_offset), (float(point.attrib['y']) - y_min_offset)])\n",
    "                allStrokes.append(points)\n",
    "            return allStrokes    \n",
    "                \n",
    "    \n",
    "        def get_all_files():\n",
    "            rootDir = \"./data\"\n",
    "            filePaths = []\n",
    "            for dirpath, dirnames, filenames in os.walk(rootDir):\n",
    "                for file in filenames:\n",
    "                    filePaths.append(os.path.join(dirpath, file))\n",
    "            return filePaths\n",
    "        \n",
    "        def get_ascii(filename, linenumber):\n",
    "            with open(filename, \"r\") as f:\n",
    "                s = f.read()\n",
    "            csr = s.find(\"CSR\")    \n",
    "            s = s[csr:]\n",
    "            if len(s.split(\"\\n\")) > line_number+2:\n",
    "                s = s.split(\"\\n\")[line_number+2]\n",
    "                return s\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "        \n",
    "    # Function to convert strokes to inputStrokeMatrix\n",
    "        def strokes_to_input_matrix(strokes):\n",
    "            strokeMatrix = []\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            for stroke in strokes:\n",
    "                for num_point in range(len(stroke)):\n",
    "                    x = stroke[num_point][0] - prev_x\n",
    "                    y = stroke[num_point][1] - prev_y\n",
    "                    prev_x = stroke[num_point][0]\n",
    "                    prev_y = stroke[num_point][1]\n",
    "                    z = 0\n",
    "                    if (num_point == len(stroke)-1):\n",
    "                        z = 1\n",
    "                    example = [x,y,z]\n",
    "                    strokeMatrix.append(example)\n",
    "            return strokeMatrix\n",
    "        \n",
    "        allFiles = get_all_files()\n",
    "        strokes = []\n",
    "        asciis = []\n",
    "        counter = 0\n",
    "        for file in allFiles:\n",
    "            if file[-3:] == \"xml\" and 'a0' in file:\n",
    "                counter = counter + 1\n",
    "                stroke = strokes_to_input_matrix(convert_linestroke_file_to_array(file))\n",
    "                # Getting corresponding ascii file and line number for stroke.\n",
    "                ascii_filename = file.replace(\"lineStrokes\", \"ascii\")[:-7] + \".txt\"\n",
    "                # print(file, ascii_filename)\n",
    "                line_number = file[-6:-4]\n",
    "                line_number = int(line_number) - 1\n",
    "                c = get_ascii(ascii_filename, line_number)\n",
    "                if (len(c) > 10):\n",
    "                    asciis.append(c)\n",
    "                    strokes.append(stroke)\n",
    "            # assert len(strokes) == counter    \n",
    "        assert len(strokes) == len(asciis)\n",
    "        f = open(data_file,\"wb\")\n",
    "        pickle.dump([strokes, asciis], f, protocol=2)\n",
    "        f.close()\n",
    "        print(\"Saved {} lines\", len(strokes))\n",
    "         \n",
    "        \n",
    "def convert_to_one_hot(s, ascii_steps, alphabet):\n",
    "    ascii_steps = int(ascii_steps)\n",
    "    steplimit=3e3; s = s[:3e3] if len(s) > 3e3 else s # clip super-long strings\n",
    "    seq = [alphabet.find(char) + 1 for char in s]\n",
    "    if len(seq) >= ascii_steps:\n",
    "        seq = seq[:ascii_steps]\n",
    "    else:\n",
    "        seq = seq + [0]*(ascii_steps - len(seq))\n",
    "    one_hot = np.zeros((ascii_steps,len(alphabet)+1))\n",
    "    one_hot[np.arange(ascii_steps),seq] = 1\n",
    "    return one_hot\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved {} lines 949\n",
      "Number of data examples: 901\n",
      "Batch size for dataset 300\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args['batch_size'] = 3\n",
    "args['tsteps'] = 100\n",
    "args['scale_factor'] = 2\n",
    "args['gap'] = 4\n",
    "args['tsteps_per_char'] = 10\n",
    "args['alphabet'] = 'abcdefgh'\n",
    "D = DataProcess(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y , asciis, asciis_oh = D.get_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the Central African Federation, went to Chequers ', 'of the Central African', 'goals and two tries to a try.']\n"
     ]
    }
   ],
   "source": [
    "print(asciis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
