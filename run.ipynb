{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved {} lines 949\n",
      "Number of data examples: 901\n",
      "Batch size for dataset 300\n",
      "['ter who wants to have a few', 'PThe bow tie has gone; he is having', 'are likely to turn down the Foot-Griffiths ']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "from model import *\n",
    "from importlib import reload\n",
    "from dataloader import DataProcess\n",
    "from sample import *\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_args():\n",
    "    \n",
    "        args = {}\n",
    "        args['rnn_size'] = 100 \n",
    "        args['tsteps'] = 300 \n",
    "        args['batch_size'] = 32 \n",
    "        args['num_batches'] = 500 \n",
    "        args['num_mixtures'] = 20 # number of MDN mixtures\n",
    "        args['window_mixtures'] = 10 # number of attention window mixtures\n",
    "        args['learning_rate'] = 0.001\n",
    "        args['epochs'] = 2500 \n",
    "        args['alphabet'] = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        args['tsteps_per_char'] = 25\n",
    "        \n",
    "        args['biases'] = 1.0\n",
    "        args['data_dir'] = \"./data\"\n",
    "        args['logs_dir'] = './logs/'\n",
    "        args['save_path'] = 'model9/model.ckpt' # path to save the model at\n",
    "        args['load_path'] = 'model8/model.ckpt' # path to load the model from\n",
    "        args['grad_clip'] = 10\n",
    "        args['n_to_save'] = 500 #step difference at which to save the model\n",
    "        args['scale_factor'] = 20\n",
    "        args['gap'] = 500 #remove data with gap greater than this threshhold\n",
    "        args['learning_rate_decay'] = 0.99 \n",
    "        args['keep_prob'] = 0.85 # keep_prob for dropout\n",
    "        args['train'] = False\n",
    "        args['decay'] = 0.95\n",
    "        args['momentum'] = 0.9\n",
    "        return args\n",
    "\n",
    "def load_pretrained_model(model, path):\n",
    "        global_step = 0\n",
    "        try:\n",
    "            save_dir = '/'.join(path.split('/')[:-1])\n",
    "            ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "            load_path = ckpt.model_checkpoint_path\n",
    "            model.saver.restore(model.sess, load_path)\n",
    "            #load_was_success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            load_was_success = False\n",
    "        else:\n",
    "            model.saver = tf.train.Saver(tf.global_variables())\n",
    "            global_step = int(load_path.split('-')[-1])\n",
    "            load_was_success = True\n",
    "        return load_was_success, global_step\n",
    "    \n",
    "def train_model():\n",
    "    args = init_args()\n",
    "    args['train'] = True\n",
    "    data_loader = DataProcess(args)\n",
    "    \n",
    "    # num_batches is calculated in dataloader based on total data size and batch_size\n",
    "    args['num_batches'] = data_loader.num_batches\n",
    "    print(\"num_bacthes\", args['num_batches'])\n",
    "    \n",
    "    model = Model(args)\n",
    "    global_step = 0\n",
    "    # load_was_success, global_step = load_pretrained_model(model, args['save_path'])\n",
    "    \n",
    "    # list to hold the loss values in each itration of the mini batch\n",
    "    plot_loss = []\n",
    "    model.sess.run(tf.assign(model.decay, args['decay']))\n",
    "    model.sess.run(tf.assign(model.momentum, args['momentum']))\n",
    "    \n",
    "    # Mini batch for given number of epochs\n",
    "    for e in range(int(global_step/args['num_batches']), args['epochs']):\n",
    "        print(\"Running epoch\", e)\n",
    "        \n",
    "        # learning rate decay\n",
    "        model.sess.run(tf.assign(model.learning_rate, args['learning_rate'] * (args['learning_rate_decay'] ** e)))\n",
    "        \n",
    "       \n",
    "        # initializes data pointer to starting of batch in each epoch\n",
    "        data_loader.init_batch_comp()\n",
    "        c0, c1, c2 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval(), model.istate_cell2.c.eval()\n",
    "        h0, h1, h2 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval(), model.istate_cell2.h.eval()\n",
    "        kappa = np.zeros((args['batch_size'], args['window_mixtures'], 1))\n",
    "\n",
    "        for b in range(global_step%args['num_batches'], args['num_batches']):\n",
    "\n",
    "            i = e * args['num_batches'] + b\n",
    "            if global_step is not 0 : i+=1 ; global_step = 0\n",
    "\n",
    "            if i % args['n_to_save'] == 0 and (i > 0):\n",
    "                # save the model we have right now\n",
    "                model.saver.save(model.sess, args['save_path'], global_step = i) ;\n",
    "            \n",
    "            # get next batch of data to train on\n",
    "            x, y, asciis, asciis_oh = data_loader.get_next_batch()\n",
    "            \n",
    "            feed = {model.input: x, model.output: y, model.char_seq: asciis_oh, model.kappa_start: kappa, \\\n",
    "                    model.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n",
    "                    model.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n",
    "            [train_loss, _] = model.sess.run([model.cost, model.train_op], feed)\n",
    "\n",
    "            plot_loss.append(train_loss)\n",
    "        print(\"train_loss: \" + str(i))\n",
    "        print(train_loss)\n",
    "    \n",
    "    # plotting the loss graph\n",
    "    plt.plot(plot_loss, linewidth=2.0)\n",
    "    plt.savefig(\"./loss.png\")\n",
    "\n",
    "# Function to sample some handwriting, this does not try to sample any particular style YET\n",
    "def sample_model():\n",
    "    args = init_args()\n",
    "    args['tsteps'] = 1\n",
    "    args['batch_size'] = 1\n",
    "\n",
    "    model = Model(args)\n",
    "    \n",
    "    # load a pretrained model\n",
    "    load_was_success, global_step = load_pretrained_model(model, args['load_path'])\n",
    "    if load_was_success:\n",
    "            strokes, char_to_plot, phis, windows, kappas = sample(model, args)\n",
    "            line_plot_coef(strokes, 'Line plot', figsize = (20,4), save_path=\"./coef_plot7.png\")\n",
    "            line_plot_char(strokes, char_to_plot, 'Line plot', figsize = (20,4), save_path=\"./line_char7.png\")\n",
    "            print(\"plotted\")\n",
    "\n",
    "    else:\n",
    "        print(\"Model failed to load, can't sample\")\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved {} lines 949\n",
      "Number of data examples: 875\n",
      "Batch size for dataset 27\n",
      "num_bacthes 27\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/shiprab/CS230-handwriting-synthesis/model.py:47: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "100 30\n",
      "WARNING:tensorflow:From /Users/shiprab/CS230-handwriting-synthesis/window.py:39: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "100 30\n",
      "WARNING:tensorflow:From /Users/shiprab/CS230-handwriting-synthesis/mdn2.py:31: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0\n",
      "train_loss: 26\n",
      "4.166103\n",
      "Running epoch 1\n",
      "train_loss: 53\n",
      "3.6340878\n",
      "Running epoch 2\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
