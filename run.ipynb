{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "from model import *\n",
    "from model2 import *\n",
    "from importlib import reload\n",
    "from dataloader import DataProcess\n",
    "from sample2 import *\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_args():\n",
    "    \n",
    "        args = {}\n",
    "        args['rnn_size'] = 100 \n",
    "        args['tsteps'] = 600 #150 #300 \n",
    "        args['batch_size'] = 128 #32 \n",
    "        args['num_batches'] = 500 \n",
    "        args['num_mixtures'] = 20 # number of MDN mixtures\n",
    "        args['window_mixtures'] = 10 # number of attention window mixtures\n",
    "        args['learning_rate'] = 0.001 \n",
    "        args['epochs'] = 3000 \n",
    "        args['alphabet'] = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        args['tsteps_per_char'] = 25\n",
    "        \n",
    "        args['biases'] = 1.0\n",
    "        args['style'] = 1\n",
    "        args['data_dir'] = \"./data\"\n",
    "        args['logs_dir'] = './logs/'\n",
    "        args['save_path'] = 'models/modelwindow7/model.ckpt' # path to save the model at\n",
    "        args['load_path'] = 'models/modelwindow7/model.ckpt' # path to load the model from\n",
    "        args['grad_clip'] = 10\n",
    "        args['n_to_save'] = 500 #step difference at which to save the model\n",
    "        args['scale_factor'] = 50\n",
    "        args['gap'] = 500 #remove data with gap greater than this threshhold\n",
    "        args['learning_rate_decay'] = 0.99  \n",
    "        args['keep_prob'] =  0.9 # keep_prob for dropout\n",
    "        args['train'] = False\n",
    "        args['decay'] = 0.9\n",
    "        args['momentum'] = 0.9\n",
    "        args['use_peepholes'] = False\n",
    "        return args\n",
    "\n",
    "def load_pretrained_model(model, path):\n",
    "        global_step = 0\n",
    "        try:\n",
    "            save_dir = '/'.join(path.split('/')[:-1])\n",
    "            ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "            load_path = ckpt.model_checkpoint_path\n",
    "            model.saver.restore(model.sess, load_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            load_was_success = False\n",
    "        else:\n",
    "            model.saver = tf.train.Saver(tf.global_variables())\n",
    "            global_step = int(load_path.split('-')[-1])\n",
    "            load_was_success = True\n",
    "        return load_was_success, global_step\n",
    "    \n",
    "def train_model():\n",
    "    args = init_args()\n",
    "    args['train'] = True\n",
    "    # args['use_peepholes'] = True\n",
    "    data_loader = DataProcess(args)\n",
    "    \n",
    "    \n",
    "    # num_batches is calculated in dataloader based on total data size and batch_size\n",
    "    args['num_batches'] = data_loader.num_batches\n",
    "    print(\"num_batches\", args['num_batches'])\n",
    "    \n",
    "    model = Model(args)\n",
    "    global_step = 0\n",
    "    \n",
    "    load_was_success, global_step = load_pretrained_model(model, args['save_path'])\n",
    "    valid_x, valid_y, valid_string, valid_oh = data_loader.get_validation_data()\n",
    "    valid_inputs = {model.input: valid_x, model.output: valid_y, model.char_seq: valid_oh}\n",
    "    \n",
    "    # list to hold the loss values in each itration of the mini batch\n",
    "    plot_loss = []\n",
    "    model.sess.run(tf.assign(model.decay, args['decay']))\n",
    "    model.sess.run(tf.assign(model.momentum, args['momentum']))\n",
    "    \n",
    "    # Mini batch for given number of epochs\n",
    "    for e in range(int(global_step/args['num_batches']), args['epochs']):\n",
    "        print(\"Running epoch\", e)\n",
    "        \n",
    "        # learning rate decay\n",
    "        model.sess.run(tf.assign(model.learning_rate, args['learning_rate'] * (args['learning_rate_decay'] ** e)))\n",
    "        print(\"learning rate: \", model.learning_rate.eval())\n",
    "       \n",
    "        # initializes data pointer to starting of batch in each epoch\n",
    "        data_loader.init_batch_comp()\n",
    "        c0, c1, c2 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval(), model.istate_cell2.c.eval()\n",
    "        h0, h1, h2 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval(), model.istate_cell2.h.eval()\n",
    "        kappa = np.zeros((args['batch_size'], args['window_mixtures'], 1))\n",
    "\n",
    "        for b in range(global_step%args['num_batches'], args['num_batches']):\n",
    "\n",
    "            i = e * args['num_batches'] + b\n",
    "            if global_step is not 0 : i+=1 ; global_step = 0\n",
    "\n",
    "            if i % args['n_to_save'] == 0 and (i > 0):\n",
    "                # save the model we have right now\n",
    "                model.saver.save(model.sess, args['save_path'], global_step = i) ;\n",
    "            \n",
    "            # get next batch of data to train on\n",
    "            x, y, asciis, asciis_oh = data_loader.get_next_batch()\n",
    "            \n",
    "            feed = {model.input: x, model.output: y, model.char_seq: asciis_oh, model.kappa_start: kappa, \\\n",
    "                    model.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n",
    "                    model.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n",
    "            [train_loss, _] = model.sess.run([model.cost, model.train_op], feed)\n",
    "            \n",
    "            # Running model for validation inputs\n",
    "            feed.update(valid_inputs)\n",
    "            feed[model.kappa_start] = np.zeros((args['batch_size'], args['window_mixtures'], 1))\n",
    "            [valid_loss] = model.sess.run([model.cost], feed)\n",
    "\n",
    "            plot_loss.append(train_loss)\n",
    "        print(\"train_loss: \", train_loss)\n",
    "        print(\"Valid Loss: \", valid_loss)\n",
    "    \n",
    "    # plotting the loss graph\n",
    "    plt.plot(plot_loss, linewidth=2.0)\n",
    "    plt.savefig(\"./loss.png\")\n",
    "\n",
    "def train_model_two_layer():\n",
    "    args = init_args()\n",
    "    args['train'] = True\n",
    "    data_loader = DataProcess(args)\n",
    "    \n",
    "    \n",
    "    # num_batches is calculated in dataloader based on total data size and batch_size\n",
    "    args['num_batches'] = data_loader.num_batches\n",
    "    print(\"num_batches\", args['num_batches'])\n",
    "    \n",
    "    model = Model_two_layer(args)\n",
    "    load_was_success, global_step = load_pretrained_model(model, args['save_path'])\n",
    "    valid_x, valid_y, valid_string, valid_oh = data_loader.get_validation_data()\n",
    "    valid_inputs = {model.input: valid_x, model.output: valid_y, model.char_seq: valid_oh}\n",
    "    \n",
    "    # list to hold the loss values in each itration of the mini batch\n",
    "    plot_loss = []\n",
    "    model.sess.run(tf.assign(model.decay, args['decay']))\n",
    "    model.sess.run(tf.assign(model.momentum, args['momentum']))\n",
    "    \n",
    "    # Mini batch for given number of epochs\n",
    "    for e in range(int(global_step/args['num_batches']), args['epochs']):\n",
    "        print(\"Running epoch\", e)\n",
    "        \n",
    "        # learning rate decay\n",
    "        model.sess.run(tf.assign(model.learning_rate, args['learning_rate'] * (args['learning_rate_decay'] ** e)))\n",
    "        print(\"learning rate: \", model.learning_rate.eval())\n",
    "       \n",
    "        # initializes data pointer to starting of batch in each epoch\n",
    "        data_loader.init_batch_comp()\n",
    "        c0, c1 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval()\n",
    "        h0, h1 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval()\n",
    "        kappa = np.zeros((args['batch_size'], args['window_mixtures'], 1))\n",
    "\n",
    "        for b in range(global_step%args['num_batches'], args['num_batches']):\n",
    "\n",
    "            i = e * args['num_batches'] + b\n",
    "            if global_step is not 0 : i+=1 ; global_step = 0\n",
    "\n",
    "            if i % args['n_to_save'] == 0 and (i > 0):\n",
    "                # save the model we have right now\n",
    "                model.saver.save(model.sess, args['save_path'], global_step = i) ;\n",
    "            \n",
    "            # get next batch of data to train on\n",
    "            x, y, asciis, asciis_oh = data_loader.get_next_batch()\n",
    "            \n",
    "            feed = {model.input: x, model.output: y, model.char_seq: asciis_oh, model.kappa_start: kappa, \\\n",
    "                    model.istate_cell0.c: c0, model.istate_cell1.c: c1, \\\n",
    "                    model.istate_cell0.h: h0, model.istate_cell1.h: h1}\n",
    "            [train_loss, _] = model.sess.run([model.cost, model.train_op], feed)\n",
    "            \n",
    "            # Running model for validation inputs\n",
    "            feed.update(valid_inputs)\n",
    "            feed[model.kappa_start] = np.zeros((args['batch_size'], args['window_mixtures'], 1))\n",
    "            [valid_loss] = model.sess.run([model.cost], feed)\n",
    "\n",
    "            plot_loss.append(train_loss)\n",
    "        print(\"train_loss: \", train_loss)\n",
    "        print(\"Valid Loss: \", valid_loss)\n",
    "        \n",
    "    \n",
    "    # plotting the loss graph\n",
    "    plt.plot(plot_loss, linewidth=2.0)\n",
    "    plt.savefig(\"./loss.png\")\n",
    "    model.sess.close()\n",
    "\n",
    "# Function to sample some handwriting, this does not try to sample any particular style YET\n",
    "def sample_model():\n",
    "    args = init_args()\n",
    "    args['train'] = False \n",
    "\n",
    "    model = Model(args)\n",
    "    #model = Model_two_layer(args)\n",
    "    \n",
    "    # load a pretrained model\n",
    "    s = 'Jon Snow knows nothing'\n",
    "    \n",
    "    # calculate number of tsteps to plot depending on length of input\n",
    "    args['tsteps'] = len(s)*args['tsteps_per_char']\n",
    "    load_was_success, global_step = load_pretrained_model(model, args['load_path'])\n",
    "    if load_was_success:\n",
    "            strokes, char_to_plot, phis, windows, kappas = sample(s, model, args) # sample(s, model, args)\n",
    "            window_plots(phis, windows, save_path=\"./window_plot7.png\")\n",
    "            line_plot_coef(strokes, 'Line plot', figsize = (20,4), save_path=\"./coef_plot7.png\")\n",
    "            line_plot_char(strokes, char_to_plot, 'Line plot', figsize = (20,4), save_path=\"./line_char7.png\")\n",
    "            print(\"plotted\")\n",
    "\n",
    "    else:\n",
    "        print(\"Model failed to load, can't sample\")\n",
    "    \n",
    "    if True:\n",
    "        # keep sampling until stopped\n",
    "        tf.reset_default_graph()\n",
    "        time.sleep(4)\n",
    "        sample_model()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_model_two_layer()\n",
    "sample_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
