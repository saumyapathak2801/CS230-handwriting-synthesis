{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from mdn2 import *\n",
    "from window import *\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.initializer = tf.truncated_normal_initializer(mean=0., stddev=.075, seed=None, dtype=tf.float32)\n",
    "        self.learning_rate = args['learning_rate']\n",
    "        self.tsteps = args['tsteps'] if args['train'] else 1\n",
    "        self.num_mixtures = args['num_mixtures']\n",
    "        self.window_mixtures = args['window_mixtures']\n",
    "        self.rnn_size = args['rnn_size']\n",
    "        self.batch_size = args['batch_size'] if args['train'] else 1\n",
    "        self.biases = args['biases']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.keep_prob = args['keep_prob'] \n",
    "        self.train = args['train']\n",
    "        self.char_steps = args['tsteps'] / args['tsteps_per_char']\n",
    "        self.vocab_len = len(args['alphabet']) + 1\n",
    "        \n",
    "        # Build an LSTM cell, each cell has rnn_size number of units\n",
    "        with tf.variable_scope(tf.get_variable_scope(),reuse=False):\n",
    "            cell_func = tf.contrib.rnn.LSTMCell\n",
    "            self.cell0 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.initializer, use_peepholes = args['use_peepholes'])\n",
    "            self.cell1 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.initializer, use_peepholes = args['use_peepholes'])\n",
    "            self.cell2 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.initializer, use_peepholes = args['use_peepholes'])\n",
    "            if (self.train and self.keep_prob < 1): \n",
    "                self.cell0 = tf.contrib.rnn.DropoutWrapper(self.cell0, output_keep_prob = self.keep_prob)\n",
    "                self.cell1 = tf.contrib.rnn.DropoutWrapper(self.cell1, output_keep_prob = self.keep_prob)\n",
    "                self.cell2 = tf.contrib.rnn.DropoutWrapper(self.cell2, output_keep_prob = self.keep_prob)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Placeholders for input and output data, each entry has tsteps points at a time\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n",
    "        self.output = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n",
    "        \n",
    "        # Setting the states of memory cells in each LSTM cell.\n",
    "        # batch_size is the number of training examples in a batch. Each training example is a set of tsteps number of\n",
    "        # (x,y, <end_of_stroke>) tuples, i.e. a sequence of strokes till t time steps.\n",
    "        self.istate_cell0 = self.cell0.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        self.istate_cell1 = self.cell1.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        self.istate_cell2 = self.cell2.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        \n",
    "        # Input to model is a set of batch_size number of training samples. Step below splits by tsteps, giving one element in a tstep worth in each batch\n",
    "        input_to_model = [tf.squeeze(input_, [1]) for input_ in tf.split(self.input, self.tsteps, 1)]\n",
    "        \n",
    "        def build_computational_graph(self, inputs, cell, initial_cell_state, scope):\n",
    "            output, cell_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_cell_state, cell, loop_function=None, scope=scope)\n",
    "            return [output, cell_final_state]\n",
    "        \n",
    "        outs_layer0, self.cell0_final_state = build_computational_graph(self, input_to_model, self.cell0, self.istate_cell0, 'cell0')\n",
    "        \n",
    "        # Send output of cell 0 to attention window.\n",
    "        self.kappa_start = tf.placeholder(dtype = tf.float32, shape = [None, self.window_mixtures, 1])\n",
    "        self.char_seq = tf.placeholder(dtype = tf.float32, shape = [None, self.char_steps, self.vocab_len])\n",
    "        \n",
    "        prev_kappa = self.kappa_start\n",
    "        prev_window = self.char_seq[:,0,:]\n",
    "        reuse = False\n",
    "        \n",
    "        for i in range(len(outs_layer0)):\n",
    "            alpha, beta, kappa = get_window_coefficients(outs_layer0[i], self.window_mixtures, prev_kappa, self.initializer, reuse)\n",
    "            window, phi = build_gaussian_window(alpha, beta, kappa, self.char_seq)\n",
    "            # Concatenating output of first hidden layer with window and input to send to second layer.\n",
    "            outs_layer0[i] = tf.concat((outs_layer0[i], window), 1)\n",
    "            outs_layer0[i] = tf.concat((outs_layer0[i], input_to_model[i]), 1)\n",
    "            prev_kappa = kappa\n",
    "            prev_window = window\n",
    "            reuse = True\n",
    "            \n",
    "        #save some attention mechanism params.\n",
    "        self.window = window\n",
    "        self.phi = phi # Window Weight of c \n",
    "        self.kappa = kappa # Controls location\n",
    "        self.alpha = alpha # Stores importance of window within mixture\n",
    "        self.beta = beta # Controls width\n",
    "        \n",
    "        outs_layer1, self.cell1_final_state = build_computational_graph(self, outs_layer0, self.cell1, self.istate_cell1, 'cell1')\n",
    "        outs_layer2, self.cell2_final_state = build_computational_graph(self, outs_layer1, self.cell2, self.istate_cell2, 'cell2')\n",
    "        \n",
    "        # The output of final layer goes into MDN\n",
    "        # for each output we predict 6 parameters + 1 eos for \"num_mixtures\" mixture density components\n",
    "        total_mdn_params = 6 * self.num_mixtures + 1\n",
    "        \n",
    "        # according to eq(17) in https://arxiv.org/pdf/1308.0850.pdf\n",
    "        with tf.variable_scope('mdn_dense'):\n",
    "            # initializing W and b matrix for eq(17)\n",
    "            W_to_mdn = tf.get_variable(\"output_w\", [self.rnn_size, total_mdn_params], initializer=self.initializer)\n",
    "            b_to_mdn = tf.get_variable(\"output_b\", [total_mdn_params], initializer=self.initializer)\n",
    "            \n",
    "        outs_layer2 = tf.reshape(tf.concat(outs_layer2, 1), [-1, args['rnn_size']])\n",
    "        output_layer = tf.nn.xw_plus_b(outs_layer2, W_to_mdn, b_to_mdn) #eq(17) in https://arxiv.org/pdf/1308.0850.pdf\n",
    "        flat_output = tf.reshape(self.output,[-1, 3])\n",
    "        [output_x, output_y, eos_data] = tf.split(flat_output, 3, 1)\n",
    "        \n",
    "        # MDN\n",
    "        [self.eos, self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho] = get_mdn_coef(self, output_layer)\n",
    "        loss = get_loss(self.pi, output_x, output_y, eos_data, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho, self.eos)\n",
    "        self.cost = loss / (self.batch_size * self.tsteps) # J = 1/m*sum(Loss) , m = number of training examples\n",
    "        \n",
    "        # Using RMSProp optimizer for optimizing the cost, defining it's parameters\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        self.decay = tf.Variable(0.0, trainable=False)\n",
    "        self.momentum = tf.Variable(0.0, trainable=False)\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=self.decay, momentum=self.momentum)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), self.grad_clip)\n",
    "        self.train_op = self.optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        # saver for saving the model's variables\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
